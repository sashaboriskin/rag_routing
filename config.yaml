generation_config:
    do_sample: false
    temperature: 1
    top_p: 1
    
model_id: "meta-llama/Llama-3.2-3B-Instruct"
seed: 42
